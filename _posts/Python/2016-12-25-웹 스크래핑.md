---
layout: post
title: Web Scraping
date: 2016-12-25
tags:
  - Python
---

다음은 Al Sweigart의 Automate the Boring Stuff with Python의 Chapter 11 – Web Scraping을 한국어로 번역하고 파이썬 2.7로 바꾼 것입니다. 원저자가 지정한 Creative Commons License에 의해 자유롭게 수정 후 재배포가 가능하며 출처는 표기해야 합니다.

# 웹 스크래핑(Web Scraaping)

컴퓨터에서 돌아가는 수많은 프로그램들은 실제로 인터넷 위에서 돌아가기 때문에 프로그램이 온라인에 접속하는 것은 꽤 중요하다. **웹 스크래핑**은 프로그램이 웹에서 컨텐츠를 다운로드하고 처리할 수 있게 하는 것을 말한다. 예를 들어, 구글은 검색 엔진이 웹 페이지를 인덱싱하기 위해 수많은 웹 크롤러 프로그램을 실행한다. 이번 장에서, 우리는 파이썬에서 웹 페이지를 쉽게 긁어오는 여러 모듈에 대해 배울 것이다.

1. webbrowser: 파이썬 내장 모듈로 브라우저에게 특정 페이지를 열게 할 수 있다.
2. Requests: 인터넷 상에서 웹페이지와 파일을 다운로드 하게 해준다.
3. Beautiful Soup: 웹페이지가 써져있는 형식인 HTML을 파싱한다.

# 프로젝트: webbrower를 사용한 MAPIT.PY

webbrower 모듈의 open함수는 브라우저에게 특정 URL을 실행시킬 수 있게 해준다. 다음을 실행시켜보자.


```python
import webbrowser
webbrowser.open('http://inventwithpython.com/')
```




    True



이게 webbrowser가 할 수 있는 유일한 것이다. 그럼에도 불구하고, open함수는 여러가지 흥미로운 것들을 가능하게 해준다. 예를 들어, 도로 주소를 클립보드에 복사해서 구글 맵에 입력하는 일은 귀찮다. 우리는 클립 보드의 컨텐츠를 사용해서 우리의 브라우저에 있는 지도를 자동으로 실행시키는 단순한 스크립트를 작성함으로써 이 작업의 몇 가지 단계를 생략할 수 있다. 이 방법으로, 우리가 해야하는 일은 클립 보드에 주소를 복사하고, 스크립트를 실행하는 것 뿐이다. 그렇게 하면 해당 위치의 지도가 바로 뜰 것이다.

우리의 프로그램이 하는 일은 다음과 같다.
1. 명령프롬프트나 클립 보드에서 주소를 얻기
2. 해당 주소의 구글 맵을 웹 브라우저에게 열게하기

이는 우리의 코드가 다음 일들을 하는 것을 뜻한다.
1. sys.argv로부터 명령프롬프트를 읽기
2. 클립보드 읽기
3. 웹 브라우저를 열기 위해 webbrowser.open 함수 호출하기

# 1단계: URL 구성하기

첫째로 우리는 주어진 도로의 주소를 사용하기 위해 어떤 URL을 사용해야하는지 알아야 한다.<br>
http://maps.google.com/ 을 브라우저에서 실행해서 주소를 검색하면, 주소창의 URL은 다음과 같을 것이다:

https://www.google.com/maps/place/870+Valencia+St/@37.7590311,-122.4215096,17z
(이어서)/data=!3m1!4b1!4m2!3m1!1s0x808f7e3dadc07a37:0xc86b0b2bb93b73d8.

주소가 URL안에 있다, 하지만 URL에는 수많은 부가적인 정보가 달려있다. 웹사이트들은 때때로 경로를 찾는 방문자나 사이트를 개조하기 위해 부가적인 데이터를 URL에 넣곤한다. 하지만 https://www.google.com/maps/place/870+Valencia+St+San+Francisco+CA/ 라고만 입력해도 올바른 페이지를 가져온다는 것을 알 수 있다. 따라서 우리의 프로그램은 다음과 같이 입력해주면 충분하다.

'https://www.google.com/maps/place/your_address_string' (your_address_string 은 우리가 찾고 싶은 주소).

# 2단계: 명령 프롬프트 변수 다루기

다음 코드를 보자.


```python
# mapIt.py - Launches a map in the browser using an address from the
# command line or clipboard.

import webbrowser, sys
if len(sys.argv) > 1:
    # Get address from command line.
    address = ' '.join(sys.argv[1:])

# TODO: Get address from clipboard.
```

주석 뒤에, 우리는 브라우저를 실행하기 위해 webbrowser 모듈을 임포트 해야하며, 잠재적인 명령 프롬프트 변수를 읽기 위해 sys 모듈을 임포트 해야한다. sys.argv 변수는 프로그램의 파일명과 명령 프롬프트의 변수의 리스트를 저장한다. 만약 이 리스트가 파일명 이외의 정보를 가지고 있다면, len(sys.argv)는 길이거 1보다 큰 정수가 될 것이며, 이는 명령 프롬프트의 변수가 저장되었다는 뜻이다.

명령 프롬프트 변수는 일반적으로 공백으로 구분되지만, 이번 경우에 우리는 몯느 변수를 하나의 문자열로 해석하고 싶다. sys.argv가 문자열의 리스트이기 때문에, 우리는 이를 하나의 문자열로 리턴하는 join함수를 쓰면 된다. 우리는 문자열에서 프로그램의 이름은 필요하지 않기 때문에 sys.argv[1:]를 인풋으로 넣으면 된다. 그리고 이렇게 조인한 문자열을 address 변수에 저장하면 된다.

만약 우리가 다음 코드를 명령 프롬프트 창에 입력하면

mapit 870 Valencia St, San Francisco, CA 94110

sys.argv 변수는 다음과 같은 리스가 될 것이다.

['mapIt.py', '870', 'Valencia', 'St, ', 'San', 'Francisco, ', 'CA', '94110']

그리고 우리의 address 변수는 다음과 같을 것이다.

'870 Valencia St, San Francisco, CA 94110'

# 3단계: 클립보드 컨텐츠를 다루고 브라우저를 실행하기

위 코드를 다음과 같이 추가하자.


```python
# mapIt.py - Launches a map in the browser using an address from the
# command line or clipboard.

import webbrowser, sys, pyperclip
if len(sys.argv) > 1:
    # Get address from command line.
    address = ' '.join(sys.argv[1:])
else:
    # Get address from clipboard.
    address = pyperclip.paste()

webbrowser.open('https://www.google.com/maps/place/' + address)
```




    True



명령 프롬프트 변수가 없다면, 프로그램은 클립 보드에 저장된 주소를 가정할 것이다. 우리는 클립보드 상의 내용을 pyperclip.paste 함수로 얻을 수 있으며 이를 address 변수에 저장할 수 있다. 마지막으로 webbrowser.open 함수를 사용해서 웹 브라우저를 실행시키면 된다.

어떤 프로그램들은 많은 작업을 함으로써 몇시간을 절약시켜주기도 하지만, 해당하는 주소의 지도를 열어주는 것처럼 매번 할때마다 몇 초씩 절약하는 프로그램도 그만큼 쓸모가 있다.

# requests 모듈을 사용해서 웹에서 파일을 다운로드 받기

requests 모듈은 네트워크 에러, 연결 문제, 데이터 압축 등의 복잡한 문제들을 걱정할 필요 없이 웹에서 쉽게 파일들을 다운로드 받게 해준다. requests 모듈은 파이썬 내장 모듈이 아니기 때문에 먼저 다운로드 받아야 한다. 명령 프롬프트에 다음과 같이 입력하자.(아나콘다에는 이미 깔려있는 것 같습니다.)

pip install requests

이제 다음 코드를 실행해보자.


```python
import requests
```

# requests.get 함수를 사용해서 웹 페이지를 다운로드 받기

requests.get 함순느 다운로드하기 위해서 URL 문자열을 인풋으로 받는다. requests.get 함수의 리턴값에 type함수를 적용해보면 우리는 우리의 request에 대한 응답으로 웹서버의 response를 포함하는 Response 객체를 리턴값을 받는 다는 것을 알 수 있다. Respone 객체에 대해서 나중에 자세히 설명하기로 하고, 인터넷이 연결되어있다는 가정하에 다음 코드를 실행하자.


```python
res = requests.get('https://automatetheboringstuff.com/files/rj.txt')
print type(res)
print res.status_code == requests.codes.ok
print len(res.text)
print res.text[:250]
```

    <class 'requests.models.Response'>
    True
    174130
    ï»¿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare

    This eBook is for the use of anyone anywhere at no cost and with
    almost no restrictions whatsoever.  You may copy it, give it away or
    re-use it under the terms of the Project


위 코드의 URL은 로미오와 줄리엣의 대본 웹페이지로 이동한다. 우리는 response 객체의 status_code 속성 값을 reqeusts.code.ok 와 비교함으로써 웹 페이지에 대한 request가 정상적으로 처리되었는지 확인할 수 있다.(여기서 스테이터스 코드는 200이 성공이라는 뜻인데, 우리는 이미 404 NOT FOUND 오류에 익숙할 것이다. 이것도 이 스테이터스 코드이다!)

만약 요청이 성공적이었다면, 다운로드된 웹페이지는 문자열 형식으로 Response 객체의 text 변수로 저장되어 있을 것이다.위 예제에서는 250글자만 뽑아온 것이다.

# 에러 확인하기

우리가 이미 확인한대로 다운로드가 성공적인지 확인하기 위해 requests.codes.ok와 스테이터스 코드가 같은지 확인하면 됐었다. 더 간단하게 성공인지 확인하는 방법은 Response객체의 raise_for_status 함수를 사용하는 것이다. 이 함수는 파일을 다운로드 하는 중에 오류가 있었다면 예외를 내보내고 성공적으로 다운로드를 했다면 아무일도 일어나지 않는다. 다음 코드를 실행해보자.


```python
res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
```
res.raise_for_status()
raise_for_status 함수는 불량 다운로드가 발생했을때 프로그램이 중지하는 것을 보장하는 좋은 방법이다. 이는 중요한데, 우리는 예상치 못한 에러가 일어났을때 프로그램을 최대한 빨리 종료하길 원하기 때문이다. 만약 실패한 다운로드가 그닥 중요하지 않다면, 우리는 raise_for_status를 예외문으로 처리할 수도 있다.


```python
import requests
res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
try:
    res.raise_for_status()
except Exception as exc:
    print('There was a problem: %s' % (exc))
```

    There was a problem: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist


raise_for_status함수는 requests.get 함수가 호출된 뒤에 꼭 호출되어야 한다. 우리는 프로그램이 계속해서 진행되기 전에 실제로 다운로드가 잘 되었는지 확인해야 한다!

# 다운로드한 파일을 하드 드라이브에 저장하기

여기서부터, 우리는 내장 open함수와 write함수를 사용해서 웹페이지를 하드 드라이브에 저장할 수 있다. 첫째로, open함수의 두번째 변수에 wb문자열을 사용해서 바이너리 작성 모드로 파일을 열어야 한다. 페이지가 일반 텍스트일때도, 텍스트의 유니코드 인코딩을 유지하기 위해 우리는 텍스트 데이터 대신에 바이너리 데이터를 작성할 필요가 있다. 유니코드 인코딩은 이 스터디의 범위를 넘어서지만 다음 웹 페이지를 통해 배울 수 있다.

http://www.joelonsoftware.com/articles/Unicode.html
http://nedbatchelder.com/text/unipain.html

웹페이지를 파일로 작성하기 위해서, 우리는 Response 객체의 iter_content 함수와 함께 for문을 사용할 수 있다.


```python
res = requests.get('https://automatetheboringstuff.com/files/rj.txt')
res.raise_for_status()
playFile = open('RomeoAndJuliet.txt', 'wb')
for chunk in res.iter_content(100000):
        playFile.write(chunk)
playFile.close()
```

iter_contetn함수는 루프의 매 반복마다 내용의 '덩어리'를 리턴한다. 각 덩어리는 바이트 데이터 타입이고, 우리는 각 덩어리가 몇 바이트인지 지정할 수 있다. 10만 바이트는 일반적으로 괜찮은 사이즈이기 때문에 여기서는 10만 바이트를 지정했다.

RomeoAndJuliet.txt 파일은 이제 우리의 폴더에 존재한다. 여기서 웹사이트 상에서 파일 이름은 rj.txt였지만 우리의 하드 드라이브 속 파일은 다른 이름임에 주목하라. requests모듈은 단순히 웹페이지의 내용을 다운로드하는 것을 다룰 뿐이다. 페이자가 다운로드된 다음에는 우리 프로그램 상의 데이터가 될 뿐이다. 웹 페이지를 다운로드 받은 다음에 인터넷 연결이 끊어지더라도, 페이지의 데이터는 컴퓨터에 남아있다.

정리하면, 파일을 다운로드하고 저장하는 절차는 다음과 같다.

1. 파일을 다운로드하기 위해 requests.get 함수를 호출한다.
2. 바이너리 작성 모드로 새로운 파일을 작성하기 위해 open함수의 두번째 인풋에 wb를 넣어준다.
3. 객체의 iter_content 함수를 루프로 반복한다.
4. 내용을 파일로 저장하기 위해 각 반복마다 write함수를 호출한다.
5. 파일을 다 작성한 뒤에 close함수를 호출한다.

중간에 나오는 iter_content 함수는 복잡해 보일수도 있지만 requests함수는 우리가 큰 파일을 다운로드 할때도 그다지 큰 메모리를 잡아먹지 않는다. requests 모듈에 관한 다른 정보는 http://requests.readthedocs.org/ 에서 찾아볼 수 있다.

# HTML

웹 페이지를 쪼개기 전에 우리는 기초 HTML먼저 다룰 것이다. 여기서 우리는 웹에서 정보를 스크랩하는 것을 더 쉽게 만들어주는 웹 브라우저의 강력한 개발자 도구에 어떻게 접근하는지 확인할 것이다.

# HTML 학습 자료

Hypertext Markup Language(HTML)은 웹 페이지가 작성된 형식이다. 기초적인 내용을 자세히 서술하기에는 시간이 더 필요하므로 여기서는 튜토리얼의 목록만 제공한다.

http://htmldog.com/guides/html/beginner/

http://www.codecademy.com/tracks/web/

https://developer.mozilla.org/en-US/learn/html/

# 웹 페이지의 HTML 소스 보기

우리는 작업하게 될 웹 페이지의 HTML 소스를 볼 필요가 있다. 이를 하기 위해서는 웹 브라우저에서 오른쪽 클릭을 해서 **소스 보기**나 **페이지 소스 보기**에 해당하는 항목을 클릭하면 된다. 그리고 이 소스가 웹 브라우저가 실제로 받는 텍스트이다. 브라우저는 이 HTML에서 렌더링을 통해 우리가 보는 웹페이지를 구성하는 것이다.

당신이 즐겨 찾는 페이지들의 HTM 소스를 한번씩 보는 것을 추천한다. 소스를 볼때 어떤 뜻인지 완벽하게 이해하지 못하더라도 괜찮다. 우리는 단순한 스크래핑 프로그램을 만들기 위해 HTML 장인이 될 필요는 없다. 우리가 필요한건 단순히 존재하는 사이트에서 데이터를 끄집어내기 위한 지식이면 충분하다.

# 개발자 도구 열기

웹페이지의 소스를 보는 것에 더해, 우리는 페이지의 HTML을 개발자 도구를 사용해서 볼 수 있다. 크롬이나 IE에서는 F12를 누르면 보일 것이다. 다시 한 번 누르면 사라질 것이다. 맥에서는 개발자 도구가 보이게 환결설정을 한 귀에 command-option-I를 누르면 보일 것이다.

개발자 도구를 활성화 한 다음에 우리는 **요소 점검**을 선택함으로써 해당 부분의 HTML 코드를 볼 수 있다. 이는 웹 스크래핑을 할때 HTML을 파싱하기 시작할때 중요하다.

또한, HTML을 파싱할때 정규 표현식을 사용하지 말아야 한다.

문자열에서 특정 조각의 HTML을 찾는 것은 정규 표현식의 사용법과 정확하게 일치하는 것처럼 보인다. 하지만, 쓰지 않기를 추천한다. 그 이유는 HTML이 구성되는 방법은 여러가지인데 이 모두가 유효한 HTML로 간주되기 때문이다. 하지만 이러한 HTML을 모두 정규표현식으로 표현하는 것은 지루하고 실수하기 쉬운 부분이다. Beautiful Soup처럼 HTML을 파싱하는데 특화된 모듈을 사용하는 것이 더 적은 버그를 생성할 것이다.

# HTML 요소를 찾기 위해 개발자 도구를 사용하기

웹 페이지를 requests 모듈로 다운 받은 뒤에, 우리는 HTML 컨텐츠를 하나의 문자열 값으로 가지고 있을 것이다. 이제 우리가 필요한 것은 우리가 흥미있는 정보가 HTML의 어떤 부분에 해당하는 것인지이다.

바로 이 부분이 개발자 도구가 도와줄 수 있는 부분이다. http://www.kma.go.kr/weather/forecast/timeseries.jsp 에서 일기예보하는 데이터를 긁어오는 프로그램을 가져오길 원한다고 해보자. 코드를 작성하기 위해 약간의 준비를 하자.

우리가 만약 기온 정보에 관심이 있다면 어떻게 해야할까? 마우스를 기온에다가 올려놓고 우클릭을 한 뒤에 요소 검사를 하면 된다. 이를 통해 우리는 개발자 도구를 볼 수있는데 여기서 기온 정보에 해당하는 HTML이 뭔지 확인할 수 있다.

개발자 도구를 통해, 우리는 기온에 해당하는 HTMl은 다음과 같음을 알 수 있다.
<dd class="now_weather1_right temp1 MB10">16℃</dd>
이것이 바로 우리가 찾던 것이다! 우리는 기온 정보가 now_weather1_right temp1 MB10 클래스의 p요소임 안에 있음을 확인했다. 이제부터 BeautifulSoup 모듈이 문자열에서 우리가 찾는 것을 도와줄 것이다.

# BeautifulSoup 모듈로 HTML 파싱하기

BeautifulSoup는 HTML페이지에서 정보를 추출하는 모듈이다. BeautifulSoup의 모듈명은 bs4이다. 이를 설치하기 위해서는 명령 프롬프트에 다음과 같이 입력하면 된다.

pip install beautifulsoup4

그리고 함수를 임포트 할때는 코드에 다음과 같이 명시하면 된다.

import bs4

그리고 첨부한 test.html을 보면 HTML파일은 다양한 태그와 속성들이 있음을 확인할 수 있다. 이는 매우 복잡한데, BeautifulSoup를 통해 훨씬 쉽게 처리할 수 있다.

# HTML로부터 BeautifulSoup 객체 만들기

bs4.BeautifulSoup 함수는 파싱할 HTML을 포함한 문자열과 함께 호출되어야 한다. bs4.BeautifulSoup 함수는 BeautifulSoup 객체를 리턴한다.


```python
import requests, bs4

res = requests.get('http://nostarch.com')
res.raise_for_status()
noStarchSoup = bs4.BeautifulSoup(res.text)
print type(noStarchSoup)
```

    <class 'bs4.BeautifulSoup'>


    /Users/mac/anaconda/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

    The code that caused this warning is on line 174 of the file /Users/mac/anaconda/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:

     BeautifulSoup([your markup])

    to this:

     BeautifulSoup([your markup], "lxml")

      markup_type=markup_type))


위 코드는 앞에서 언급한 requests.get 함수를 이용해 No Starch Press의 메인 페이지를 가져오고 bs4.BeautifulSoup 함수에 Response 객체의 text 속성을 넘긴다. 이 결과 BeautifulSoup 객체가 리턴되는데 이를 noStarchSoup에 저장한다.

이 방식 이외에도 하드 드라이브에 저장한 HTML파일을 로드할 수도 있는데 이는 bs4.BeautifulSoup 함수에 File객체를 넘기면 된다.


```python
res = requests.get('http://www.kma.go.kr/weather/forecast/timeseries.jsp')
res.raise_for_status()
playFile = open('weather.html', 'wb')
for chunk in res.iter_content(100000):
        playFile.write(chunk)
playFile.close()
```


```python
exampleFile = open('example.html')
exampleSoup = bs4.BeautifulSoup(exampleFile)
print type(exampleSoup)
```

    <class 'bs4.BeautifulSoup'>


# SELECT 함수로 원소 찾기

우리는 select 함수를 호출하고 우리가 찾는 원소의 CSS selector 문자열을 넘겨줌으로써 BeautifulSoup 객체에서 웹 페이지 원소를 얻을 수 있다. Selector들은 정규 표현식과 같다! 이것들은 일반적인 텍스트 문자열 대신에 HTML페이지에서 원하는 패턴을 특정시켜준다.

CSS selector 문법에 대한 완전한 논외는 이 스터디의 범위를 넘어감으로( http://nostarch.com/automatestuff/ 에 튜토리얼들이 소개되어 있다.) 여기서는 간단한 selector들에 대한 이야기만 하도록 하겠다.

<img src = 'selector.png'>

다양한 selector 패턴의 조합을 사용해서 섬세한 검색을 할 수 있다. 예를 들어, soup.select('p #author')는 p원소 안에 있는데 id가 author인 원소를 선택한다.

select함수는 Beautiful Soup에서 HTML원소를 나타내는 Tag의 리스트를 리턴할 것이다. 이 리스트는 Beautiful Soup객체의 HTMl에서 매칭된 모든 Tag결과를 포함하고 있다. Tag값은 그들이 나타내는 HTML태크를 보여주기 위해 str함수를 사용할 수 있다. Tag값들은 또한 attrs속성을 가지고 있는데 이는 모든 태그의 HTML 속성을 딕셔너리로 보여준다.


```python
exampleSoup = bs4.BeautifulSoup(open('example.html'),'lxml')
elems = exampleSoup.select('#author')
print type(elems)
print len(elems)
print type(elems[0])
print elems[0].getText()
print str(elems[0])
print elems[0].attrs
```

    <type 'list'>
    1
    <class 'bs4.element.Tag'>
    Al Sweigart
    <span id="author">Al Sweigart</span>
    {'id': 'author'}



```python
pElems = exampleSoup.select('p')
print len(pElems)
print str(pElems[0])
print pElems[0].getText()
print str(pElems[1])
print pElems[1].getText()
print str(pElems[2])
print pElems[2].getText()
```

    3
    <p>Download my <strong>Python</strong> book from <a href="http://inventwithpython.com">my website</a>.</p>
    Download my Python book from my website.
    <p class="slogan">Learn Python the easy way!</p>
    Learn Python the easy way!
    <p>By <span id="author">Al Sweigart</span></p>
    By Al Sweigart


# 원소의 특성으로부터 데이터 얻기

Tag객체의 get함수는 원소의 속성값에 접근하는 것을 용이하게 해준다. 이 함수는 속성 이름을 문자열로 받고 속성값을 리턴한다.


```python
spanElem = exampleSoup.select('span')[0]
```


```python
str(spanElem)
```




    '<span id="author">Al Sweigart</span>'




```python
spanElem.get('id')
```




    'author'




```python
spanElem.get('some_nonexistent_addr') == None
```




    True




```python
spanElem.attrs
```




    {'id': 'author'}



# 프로젝트: "I'm Feeling Lucky" 구글 검색

구글에 주제를 검색할때, 때때로 한번에 원하는 결과를 찾지 못할 수도 있다. 결과 링크들을 클릭할때, 나중에 읽기 위해 새로운 탭을 열어둔다. 때떄로 구글을 찾는 건 이걸로 충분할 때가 있다.-브라우저를 열고, 주제를 검색하고, 여러 링크를 하나씩 열어놓고-그리고 이 작업은 꽤 지루하다. 만약 이 작업을 단순히 명령 프롬프트에서 검색하고 프로그램이 한번에 상위 검색 결과를 자동으로 띄워준다면 편할 것이다. 이번 프로젝트는 이를 코드로 작성할 것이다.

우리의 프로그램이 해야 할 일은 다음과 같다.

1. 명령 프롬프트에서 키워드를 검색하기
2. 검색 결과 페이지를 얻기
3. 각 결과를 브라우저로 보여주기

이는 우리의 코드가 다음과 같은 일을 해야함을 의미한다.

1. sys.argv를 사용해서 명령 프롬프트 인수를 읽기
2. 검색 결과를 requests 모듈로 잡아오기
3. 각 검색 결과의 링크 찾기
4. 웹 브라우저를 열기위해 webbrowser.open 함수 호출하기

# 1단계: 명령 프롬프트 인수를 받아오고 검색 페이지에 요청하기

코딩을 하기전에, 우리는 검색 결과의 URL을 알아야 한다. 구글 검색 후의 브라우저 주소창을 보게되면
https://www.google.com/search?q=SEARCH_TERM_HERE 와 같은 형식을 하고 있음을 알수 있다. requests 모듈은 이 페이지를 다운로드하고 우리는 HTML상의 결과 링크를 Beautiful Soup를 이용해서 찾을 수 있다. 최종적으로, 우리는 브라우저 탭에서 이 링크들을 열기 위해 webbrowser를 사용할 수 있다.


```python
# lucky.py - Opens several Google search results.

import requests, sys, webbrowser, bs4

print('Googling...') # display text while downloading the Google page
res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()

# TODO: Retrieve top search result links.

# TODO: Open a browser tab for each result.
# lucky.py - Opens several Google search results.
```

    Googling...


사용자는 이 프로그램을 실행시켰을 때 명령 프롬프트 인수에 검색할 내용을 지정해야한다. 이 변수는 리스트로 sys.argv에 저장된다.

# 2단계: 모든 결과 찾기

이제 우리는 우리의 다운로드된 HTML로부터 상위 결과 링크들을 추출하기 위해 Beautiful Soup를 사용해야 한다. 하지만, 어떻게하면 올바른 selector를 찾아낼 수 잇을까? 예를 들어, 모든 a 태그 결과를 다 원하는것은 아니다, 왜냐하면 HTML에는 원하는 결과외에도 a태그가 많기 때문이다. 대신에 우리는 개발자 도구를 사용해서 결과 페이지를 관찰하고 우리가 원하는 결과의 selector만 선택할 수 있다.

Beautiful Soup를 사용한 구글 검색 뒤에, 우리는 브라우저의 개발자 도구를 사용해서 링크 원소를 관찰할 수 있다. 이는 다음처럼 복잡하다.

<img src = 'element inspect.png'>

요소가 엄청나게 복잡해 보이는건 사실 중요하지 않다. 우리가 필요한건 링크가 가진 검색 결과들이 가진 패턴이 중요한 것이다. 하지만 a 요소만으로는 페이지의 검색 결과가 아닌 a원소들과 구분하는 것이 어렵다.


```python
# lucky.py - Opens several google search results.

import requests, sys, webbrowser, bs4

print('Googling...') # display text while downloading the Google page
res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()

# Retrieve top search result links.
soup = bs4.BeautifulSoup(res.text)

# Open a browser tab for each result.
linkElems = soup.select('.r a')
```

    Googling...


a 원소의 약간 위를 관찰해보면, 다음과 같은 원소를 찾을 수 있다. h3 class = "r" 처럼 생긴 원소를 찾아볼 수 있다. HTML소스의 나머지를 살펴보면, r 클래스는 검색 결과 링크에만 사용된 것처럼 보인다. 우리는 r클래스가 CSS에서 실제로 무엇을 하는지 알 필요가 없다. 우리가 그저 우리가 찾는 a원소의 마커로써만 사용할 수 있으면 된다. 우리는 다운로드된 페이지의 HTML 소스로부터 BeautifulSoup객체를 만들수 있으며, selector .r a를 사용해서 r 클래스 안에 있는 모든 a 원소를 찾아낼 수 있다.

#  3단계: 각 결과를 웹 브라우저로 열기

이제 우리는 우리의 결과를 웹 브라우저로 열라고 하면된다. 다음 코드를 보자.


```python
# lucky.py - Opens several google search results.

import requests, sys, webbrowser, bs4

print('Googling...') # display text while downloading the Google page
res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()

# Retrieve top search result links.
soup = bs4.BeautifulSoup(res.text)

# Open a browser tab for each result.
linkElems = soup.select('.r a')
numOpen = min(5, len(linkElems))
for i in range(numOpen):
    webbrowser.open('http://google.com' + linkElems[i].get('href'))
```

    Googling...


기본적으로 우리는 웹 브라우저 모듈을 사용해서 새 탭에 5개의 검색 결과를 열 수 있다. 하지만, 우리는 때때로 검색 결과가 5개보다 적을 수도 있다. soup.select 함수는 .r a selector와 매칭되는 원소들의 리스트를 리턴하며, numOpen은 5개거나 그보다 적을 수 있다.

각 루프를 돌면서, 우리는 webbrowser.open함수를 사용해서 새로운 탭을 열수 있다. 이 때, a원소에서 href 속성값은 초기의 http://google.com 부분을 가지고 있지 않기 때문에, 우리는 이를 속성의 문자열 값에 연결시켜줘야 한다.

이제 우리는 즉석에서 다섯 개의 구글 결과를 찾아볼 수 있다!

# 프로젝트: 모든 XKCD 만화 다운로드 받기

블로그들과 다른 정규적으로 업데이트하는 웹사이트들은 맨 앞에 최신 포스트들이 있으며 이전 버튼이 있어서 이전 포스트들을 볼 수 있다. 그러면 그 포스트에는 또 이전 버튼이 있고, 이렇게 계속해서 첫번째 포스트까지 연결된다. 만약 우리가 온라인이 아닐때 이 포스트들을 읽길 원한다면, 우리는 각각의 페이지를 저장해서 모든 페이지를 직접 서핑하는 방법이 있다. 하지만 이는 지루하다! 우리는 이를 대신하는 프로그램을 만들 것이다.

XKCD (http://xkcd.com) 는 우리가 앞에서 언급한 구조와 일치하는 유명한 geek만화이다. 우리는 이전 버튼을 통해 이전 만화를 볼 수 있다. 각 만화를 직ㅈ버 저장하는 것은 엄청 오래 걸릴 것이나, 우리는 몇 분만에 이를 대신하는 프로그램을 만들 수 있다.

우리의 프로그램이 해야할 일은 다음과 같다.

1. XKCD 홈페이지를 로드한다.
2. 그 페이지의 만화를 저장한다.
3. 이전 만화 링크를 따라간다.
4. 첫번째 만화에 도달할때까지 이를 반복한다.

따라서 우리는 이러한 코드를 작성해야 한다.

1. requests 모듈을 사용해서 페이지들을 다운받는다.
2. Beautiful Soup를 사용해서 만화 이미지의 URL을 찾는다.
3. iter_content()를 사용해서 하드 드라이브에 만화 이미지를 다운로드하고 저장한다.
4. 이전 만화 링크의 URL을 찾고, 이를 반복한다.

# 1단계: 프로그램 디자인하기

개발자 도구를 열어서 페이지의 원소를 관찰하면 다음과 같은 사실을 발견할 수 있다.

1. 만화의 이미지 파일은 img 원소의 src 속성값으로 주어져있다.
2. img원소는 div id = "comic" 원소 안에 있다.
3. 이전 버튼은 prev 값을 가지는 rel 속성이다.
4. 마지막 만화의 링크는 http://xkcd.com/# 인데, 이는 더이상 이전 페이지가 없다는 뜻이다.

우리의 코드는 다음과 같다.


```python
# downloadXkcd.py - Downloads every single XKCD comic.

import requests, os, bs4

url = 'http://xkcd.com'              # starting url
if not os.path.exists('xkcd'):
    os.makedirs('xkcd')
for i in range(10):
#while not url.endswith('#'):
# TODO: Download the page.

# TODO: Find the URL of the comic image.

# TODO: Download the image.

# TODO: Save the image to ./xkcd.

# TODO: Get the Prev button's url.
    i
print 'Done.'
```

    Done.


우리는 'http://xkcd.com' 로 시작하는 url 변수를 가지고 있고 이것은 현재 페이지의 이전 링크로 이동할 때마다 갱신되어야 한다. 루프의 모든 스텝마다, 우리는 url에 있는 만화를 다운로드 할 것이다. 우리는 또한 url이 '#'로 끝날때 루프를 멈춰야 하는 것을 알고 있다.

우리는 다운로드 받은 이미지 파일을 현재 작업 디렉토리 안에 있는 폴더인 xkcd 폴더에 저장할 것이다. 먼저 이 폴더가 있는지 확인한 후, 없다면 새로운 폴더를 만들어준다.

# 2단계: 웹 페이지 다운로드하기

웹페이지를 다운로드 하는 부분을 구현해보자. 우리의 코드는 다음과 같다.


```python
# downloadXkcd.py - Downloads every single XKCD comic.

import requests, os, bs4

url = 'http://xkcd.com'              # starting url
if not os.path.exists('xkcd'):
    os.makedirs('xkcd')
for i in range(10):
#while not url.endswith('#'):
# TODO: Download the page.
    print('Downloading page %s...' % url)
    res = requests.get(url)
    res.raise_for_status()

    soup = bs4.BeautifulSoup(res.text)
# TODO: Find the URL of the comic image.

# TODO: Download the image.

# TODO: Save the image to ./xkcd.

# TODO: Get the Prev button's url.
print 'Done.'
```

    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Downloading page http://xkcd.com...
    Done.


먼저 우리가 다운로드 하는 것이 뭔지 알기 위해 URL을 프린트해준다. 그리고 request.get함수를 이용해서 다운로드를 받는다. 평소대로 우리는 requests.get함수뒤에 res.raise_for_status 함수를 호출해서 예외 상황을 대비한다. 만약 문제가 일어나지 않는다면 다운로드 받은 페이지의 텍스트로부터 BeautifulSoup 객체를 만들어준다.

# 3단계: 만화 이미지를 찾고 다운로드 하기


```python
# downloadXkcd.py - Downloads every single XKCD comic.

import requests, os, bs4

url = 'http://xkcd.com' # starting url
if not os.path.exists('xkcd'):
    os.makedirs('xkcd')
for i in range(10):
#while not url.endswith('#'):
    # Download the page.
    print('Downloading page %s...' % url)
    res = requests.get(url)
    res.raise_for_status()

    soup = bs4.BeautifulSoup(res.text,'lxml')

    # Find the URL of the comic image.
    comicElem = soup.select('#comic img')
    if comicElem == []:
        print('Could not find comic image.')
    else:
        comicUrl = 'http:' + comicElem[0].get('src')
        # Download the image.
        print('Downloading image %s...' % (comicUrl))
        res = requests.get(comicUrl)
        res.raise_for_status()

print 'Done.'
```

    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Done.


개발자 도구로 XKCD 홈페이지를 살펴보면, 우리는 만화 이미지의 img원소가 id가 comic으로 설정된 div 원소 안에 있음을 알 수 있다. 따라서 selector #comic img는 BeutifulSoup 객체로부터 올바른 img 원소를 모을 수 있게 하는 것을 알 수 있다.

때때로 XKCD 페이지는 단순히 이미지 파일이 아닌 경우가 있는데 일단 괜찮다. 만약 selector가 아무 원소도 찾지 못했다면, soup.select는 빈 리스트를 리턴할 것이다. 이렇게 되면 에러 메세지를 보여주고 계속해서 반복문을 진행한다.

아닐 경우에, selecotr는 하나의 img원소를 리턴한다. 우리는 이 img원소의 src 속성을 얻은 뒤에 이것을 requests.get 함수에 전달함으로써 만화 이미지 파일을 다운로드 할 수 있다.

# 4단계: 이미지를 저장하고 이전 만화를 찾기


```python
# downloadXkcd.py - Downloads every single XKCD comic.

import requests, os, bs4

url = 'http://xkcd.com' # starting url
if not os.path.exists('xkcd'):
    os.makedirs('xkcd')
for i in range(10):
#while not url.endswith('#'):
    # Download the page.
    print('Downloading page %s...' % url)
    res = requests.get(url)
    res.raise_for_status()

    soup = bs4.BeautifulSoup(res.text,'lxml')

    # Find the URL of the comic image.
    comicElem = soup.select('#comic img')
    if comicElem == []:
        print('Could not find comic image.')
    else:
        comicUrl = 'http:' + comicElem[0].get('src')
        # Download the image.
        print('Downloading image %s...' % (comicUrl))
        res = requests.get(comicUrl)
        res.raise_for_status()

        # Save the image to ./xkcd
        imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')
        for chunk in res.iter_content(100000):
            imageFile.write(chunk)
        imageFile.close()

    # Get the Prev button's url.
    prevLink = soup.select('a[rel="prev"]')[0]
    url = 'http://xkcd.com' + prevLink.get('href')

print('Done.')
```

    Downloading page http://xkcd.com...
    Downloading image http://imgs.xkcd.com/comics/interplanetary_experience.png...
    Downloading page http://xkcd.com/1751/...
    Downloading image http://imgs.xkcd.com/comics/movie_folder.png...
    Downloading page http://xkcd.com/1750/...
    Downloading image http://imgs.xkcd.com/comics/life_goals.png...
    Downloading page http://xkcd.com/1749/...
    Downloading image http://imgs.xkcd.com/comics/mushrooms.png...
    Downloading page http://xkcd.com/1748/...
    Downloading image http://imgs.xkcd.com/comics/future_archaeology.png...
    Downloading page http://xkcd.com/1747/...
    Downloading image http://imgs.xkcd.com/comics/spider_paleontology.png...
    Downloading page http://xkcd.com/1746/...
    Downloading image http://imgs.xkcd.com/comics/making_friends.png...
    Downloading page http://xkcd.com/1745/...
    Downloading image http://imgs.xkcd.com/comics/record_scratch.png...
    Downloading page http://xkcd.com/1744/...
    Downloading image http://imgs.xkcd.com/comics/metabolism.png...
    Downloading page http://xkcd.com/1743/...
    Downloading image http://imgs.xkcd.com/comics/coffee.png...
    Done.


이제 이미지 파일은 res 변수에 저장되었다. 우리는 이 이미지 데이터를 하드 드라이브에 작성해야 한다.

우리는 open함수에 넘길 이미지 파일의 파일명이 필요하다. comicUrl은 'http://imgs.xkcd.com/comics/heartbleed_explanation.png' 와 같은 값인데 이는 파일 경로 처럼 보인다. 그리고 우리는 comicUrl을 os.path.basename()에 넘김으로써 URL의 'heartbleed_explanation.png' 같은 마지막 부분을 얻을 것이다. 우리는 이것을 하드 드라이브에 이미지로 저장할때 파일명으로 사용할 수 있다. 우리는 이 이름을 os.path.join함수로 xkcd 폴더명과 합칠 수 있다. 이제 우리는 최종적으로 파일명을 얻었다, 우리는 바이너리 모드로 open함수를 호출할 수 있다!

앞에서 Requests 함수를 사용해서 다운로드 받은 파일을 저장하기 위해, iter_content함수를 사용한 것을 기억하자. for문 안의 코드는 이미지 데이터를 덩어리로 나눠서 작성하는 역할을 한다. 이제 이미지는 우리 하드 안에 저장되었다!

다음으로, selector a[rel="prev"]는 rel 속성값이 prev로 설정된 a원소를 의미하며, 우리는 이 a원소의 href 속성 값을 사용해서 이전 만화의 URL을 얻을 수 있다. 그리고 while반복문을 통해 전체 만화에 대해서 이를 적용할 수 있다.

이 프로젝트는 웹에서 대량의 데이터를 스크랩하기 위해 자동으로 링클르 따라가는 프로그램의 좋은 예제이다. 이외에도 Beautiful Soup의 다른 내용이 궁금하다면 다음 문서를 참고하자.

http://www.crummy.com/software/BeautifulSoup/bs4/doc/
